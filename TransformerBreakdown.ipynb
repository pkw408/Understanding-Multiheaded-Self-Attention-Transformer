{"cells":[{"cell_type":"markdown","source":"# Breaking Down a Multi-Headed Self-Attention Transformer","metadata":{"tags":[]}},{"cell_type":"code","metadata":{"tags":[]},"source":"import torch \nimport torch.nn as nn \nimport torch.nn.functional as F \nfrom torch.autograd import Variable\nfrom tqdm import tqdm, trange","outputs":[]},{"cell_type":"markdown","source":"The following figure depicts the different layers a Multi-Headed Self-Attention transformer is divided into. ","metadata":{"tags":[]}},{"cell_type":"markdown","source":"<a href=\"https://imgur.com/GmgyTLk\"><img src=\"https://i.imgur.com/GmgyTLk.png\" title=\"source: imgur.com\" alt = 'transformer in layers' height = '800' width = '100%' /></a>","metadata":{"tags":[]}},{"cell_type":"markdown","source":"Let's check the flow of data inside the network","metadata":{"tags":[]}},{"cell_type":"markdown","source":"<a href=\"https://imgur.com/qIo7euI\"><img src=\"https://i.imgur.com/qIo7euI.png\" title=\"source: imgur.com\" /></a>","metadata":{"tags":[]}},{"cell_type":"markdown","source":"We will take every block one by one and see how a batch of input get's transformed through it.","metadata":{"tags":[]}},{"cell_type":"markdown","source":"# Self-Attention Block","metadata":{"tags":[]}},{"cell_type":"markdown","source":"The following figure, depicts the functions inside a **Self-Attention** block","metadata":{"tags":[]}},{"cell_type":"markdown","source":"<a href=\"https://imgur.com/0uNKIPA\"><img src=\"https://i.imgur.com/0uNKIPA.png\" title=\"source: imgur.com\" /></a>","metadata":{"tags":[]}},{"cell_type":"markdown","source":"Before starting with the exploration of data flow and processing we need to set some parameters for like,\n\n    1. Sequence Length of the input: **seq_len**\n    2. Embedding Dimension: **emb_dim**\n    3. Token Embedding\n    4. Position Embeddings\n    5. Total Number of heads: **heads = 8**\n    6. Number of tokens: **total number of tokens in a vocabulary**","metadata":{"tags":[]}},{"cell_type":"markdown","source":"Here we are considering a single input, so there is only one batch with a single input, hence if we can assume `batch_size = 1`.","metadata":{"tags":[]}},{"cell_type":"code","metadata":{"tags":[]},"source":"seq_length = 128\nemb_dim = 256\ntorch.manual_seed(1111)\nnum_tokens = 1000\nbatch_size = 1","outputs":[]},{"cell_type":"code","metadata":{"tags":[]},"source":"token_embedding = nn.Embedding(num_embeddings = num_tokens, embedding_dim = emb_dim)\nposition_embedding = nn.Embedding(num_embeddings = seq_length, embedding_dim = emb_dim)","outputs":[]},{"cell_type":"code","metadata":{"tags":[]},"source":"input_ = torch.randint(1, 128, (batch_size, seq_length))","outputs":[]},{"cell_type":"code","metadata":{"tags":[]},"source":"input_.size()","outputs":[{"output_type":"execute_result","execution_count":97,"data":{"text/plain":"torch.Size([1, 128])"},"metadata":{}}]},{"cell_type":"markdown","source":"The summation of Token Embedding and Position embedding is being transferred to the Self-Attention Block","metadata":{"tags":[]}},{"cell_type":"code","metadata":{"tags":[]},"source":"input_tok = token_embedding(input_)","outputs":[]},{"cell_type":"code","metadata":{"tags":[]},"source":"input_tok.size()","outputs":[{"output_type":"execute_result","execution_count":99,"data":{"text/plain":"torch.Size([1, 128, 256])"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[]},"source":"b, t, e = input_tok.size()","outputs":[]},{"cell_type":"code","metadata":{"tags":[]},"source":"input_pos = position_embedding(torch.arange(t))[None, :, :].expand(b, t, e)","outputs":[]},{"cell_type":"code","metadata":{"tags":[]},"source":"input_tnsr = input_tok + input_pos","outputs":[]},{"cell_type":"code","metadata":{"tags":[]},"source":"input_tnsr.size()","outputs":[{"output_type":"execute_result","execution_count":103,"data":{"text/plain":"torch.Size([1, 128, 256])"},"metadata":{}}]},{"cell_type":"markdown","source":"### Now, let's come to the **Self-Attention** block","metadata":{"tags":[]}},{"cell_type":"markdown","source":"The attention function is mapping a _query_ to a set _key-value_ pairs to an output, and query, keys, and values are all vectors","metadata":{"tags":[]}},{"cell_type":"code","metadata":{"tags":[]},"source":"toqueries = nn.Linear(emb_dim, emb_dim * heads)\ntokeys = nn.Linear(emb_dim, emb_dim * heads)\ntovalues = nn.Linear(emb_dim, emb_dim * heads)","outputs":[]},{"cell_type":"code","metadata":{"tags":[]},"source":"keys = tokeys(input_tnsr).view(b, t, heads, e)\nvalues = tovalues(input_tnsr).view(b, t, heads, e)\nqueries = toqueries(input_tnsr).view(b, t, heads, e)","outputs":[]},{"cell_type":"code","metadata":{"tags":[]},"source":"keys.size(), values.size(), queries.size()","outputs":[{"output_type":"execute_result","execution_count":112,"data":{"text/plain":"(torch.Size([1, 128, 8, 256]),\n torch.Size([1, 128, 8, 256]),\n torch.Size([1, 128, 8, 256]))"},"metadata":{}}]},{"cell_type":"markdown","source":"Folding heads into the batch dimension","metadata":{"tags":[]}},{"cell_type":"code","metadata":{"tags":[]},"source":"keys = keys.transpose(1, 2).contiguous().view(b * heads, t, e)\nvalues = values.transpose(1, 2).contiguous().view(b * heads, t, e)\nqueries = queries.transpose(1, 2).contiguous().view(b * heads, t, e)","outputs":[]},{"cell_type":"code","metadata":{"tags":[]},"source":"keys.size(), values.size(), queries.size()","outputs":[{"output_type":"execute_result","execution_count":115,"data":{"text/plain":"(torch.Size([8, 128, 256]),\n torch.Size([8, 128, 256]),\n torch.Size([8, 128, 256]))"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[]},"source":"queries = queries / (e ** (1/4))\nkeys = keys / (e ** (1/4))","outputs":[]},{"cell_type":"markdown","source":"Scaled dot-product attention","metadata":{"tags":[]}},{"cell_type":"code","metadata":{"tags":[]},"source":"dot_p = torch.bmm(queries, keys.transpose(1, 2))","outputs":[]},{"cell_type":"code","metadata":{"tags":[]},"source":"dot_p.size()","outputs":[{"output_type":"execute_result","execution_count":120,"data":{"text/plain":"torch.Size([8, 128, 128])"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[]},"source":"dot_p","outputs":[{"output_type":"execute_result","execution_count":121,"data":{"text/plain":"tensor([[[ 5.1956e-01,  6.0915e-01, -6.4917e-01,  ...,  8.6953e-01,\n          -6.6157e-02,  7.3565e-01],\n         [ 6.6372e-01,  3.4092e-01, -8.4028e-01,  ..., -1.5064e-01,\n           5.8384e-01,  2.6677e-01],\n         [ 5.0197e-01, -1.5173e+00, -6.3352e-01,  ..., -3.9618e-01,\n          -3.3248e-01, -1.4102e+00],\n         ...,\n         [ 5.8578e-01, -2.9937e-01, -3.8499e-01,  ...,  7.2306e-01,\n          -6.8306e-01, -1.1674e+00],\n         [-1.3212e+00,  3.1136e-01,  1.0709e+00,  ...,  4.4921e-01,\n          -2.3707e-01,  5.8058e-01],\n         [ 1.0418e-02,  3.7939e-01, -2.1556e+00,  ..., -3.2824e-01,\n          -2.8023e-01,  3.1092e-01]],\n\n        [[-1.0443e+00, -2.6809e-01,  4.0622e-01,  ..., -7.8090e-02,\n           1.4918e+00, -1.8269e-01],\n         [-3.7174e-01, -1.4060e+00, -9.4674e-01,  ...,  2.1743e-02,\n          -1.0852e+00, -8.4526e-01],\n         [-5.1599e-01, -3.8519e-01, -5.2806e-01,  ..., -2.7864e-01,\n          -2.5623e-01,  3.9407e-01],\n         ...,\n         [-1.5574e+00,  4.5360e-01, -8.1903e-01,  ..., -5.5854e-01,\n           1.1303e-01, -1.3986e+00],\n         [ 5.6440e-02,  8.5395e-01,  6.0760e-01,  ..., -1.7519e-02,\n          -4.1498e-01, -1.3627e-01],\n         [-6.2339e-01,  1.0211e+00,  1.5844e-01,  ...,  2.0438e-02,\n          -4.0216e-01,  1.4347e+00]],\n\n        [[ 5.9701e-01, -1.6311e-01, -5.2444e-01,  ...,  2.9653e-01,\n          -1.2396e-01,  9.9445e-01],\n         [-2.1650e-01,  7.0080e-01,  2.5569e-01,  ...,  3.0163e-01,\n           1.7822e-01,  4.8092e-01],\n         [-1.7413e-01, -7.3452e-01, -7.3097e-01,  ...,  8.4616e-01,\n          -1.6887e-01, -7.3028e-01],\n         ...,\n         [ 2.6120e-01, -7.2821e-02,  1.0644e+00,  ...,  1.8306e-01,\n           7.7412e-01, -4.0378e-01],\n         [-1.0722e+00,  1.7677e-01, -5.6302e-01,  ..., -3.7319e-01,\n          -5.1807e-01, -4.3539e-01],\n         [-7.4333e-01,  7.8710e-01,  6.5505e-02,  ..., -2.7973e-01,\n           7.4958e-03,  1.1724e+00]],\n\n        ...,\n\n        [[-1.1716e-01,  6.9963e-01,  1.0124e-01,  ...,  1.4301e-01,\n          -2.6350e-01,  3.7949e-01],\n         [ 2.6362e-01,  5.2795e-01,  5.7006e-01,  ..., -5.7554e-02,\n           2.7744e-02, -1.4951e-01],\n         [-3.0907e-01,  5.9527e-01,  9.7894e-02,  ..., -8.7363e-01,\n           7.9877e-03, -3.0428e-01],\n         ...,\n         [ 8.8376e-01,  3.4714e-01, -1.5056e+00,  ..., -5.0659e-03,\n           3.6561e-01,  7.9440e-01],\n         [-7.6551e-02, -7.0820e-02,  6.0545e-01,  ...,  1.8852e-01,\n           1.7380e+00,  1.3017e-01],\n         [ 4.4641e-01,  1.8362e-01, -8.4715e-01,  ..., -1.2442e+00,\n           8.5651e-02,  5.4881e-02]],\n\n        [[-2.5680e-02, -8.6102e-01,  1.0417e+00,  ...,  1.0825e+00,\n          -3.3680e-01, -3.7027e-01],\n         [ 9.1183e-01,  3.7063e-01, -8.8736e-02,  ..., -7.7327e-01,\n          -3.2024e-02, -5.5204e-01],\n         [-6.2066e-01,  7.6059e-01, -5.0554e-01,  ..., -1.8451e-03,\n           4.5753e-01, -5.6952e-01],\n         ...,\n         [-4.4890e-02,  1.0274e+00, -2.9962e-01,  ..., -2.3640e-01,\n          -8.2448e-01, -1.6374e+00],\n         [ 2.3053e-01, -3.2851e-01, -7.0293e-01,  ..., -6.7380e-01,\n          -6.2452e-01,  3.4387e-01],\n         [-1.1507e+00,  6.9159e-02, -9.1338e-03,  ...,  3.2030e-01,\n          -7.1882e-01, -6.3641e-01]],\n\n        [[-1.2415e+00, -7.0377e-01, -2.5872e-01,  ..., -7.5870e-01,\n          -1.2809e-01, -1.2914e-01],\n         [-4.2323e-01,  8.7965e-01,  8.1895e-01,  ..., -1.5855e-01,\n           5.6907e-01, -4.8047e-01],\n         [ 1.0261e+00, -2.9927e-01,  2.7081e-01,  ..., -4.6406e-01,\n           6.2572e-01, -2.0697e-01],\n         ...,\n         [-2.2967e-01,  3.5686e-01,  1.4759e+00,  ..., -2.2730e-01,\n          -4.5420e-02, -1.3391e+00],\n         [-3.8059e-01,  4.5355e-02, -1.5055e-01,  ..., -1.1253e+00,\n          -3.6824e-01,  1.6978e-02],\n         [-1.1843e+00,  1.3752e+00,  1.0420e+00,  ...,  2.0490e-01,\n           3.1075e-02,  6.5718e-01]]], grad_fn=<BmmBackward>)"},"metadata":{}}]},{"cell_type":"markdown","source":"Masking the upper half of the self-attention dot-product excluding the diagonal ","metadata":{"tags":[]}},{"cell_type":"code","metadata":{"tags":[]},"source":"batch, height, width = dot_p.size()","outputs":[]},{"cell_type":"code","metadata":{"tags":[]},"source":"indexes = torch.triu_indices(height, width, offset = 0)","outputs":[]},{"cell_type":"code","metadata":{"tags":[]},"source":"maskval = float('-inf')","outputs":[]},{"cell_type":"code","metadata":{"tags":[]},"source":"dot_p[:, indexes[0], indexes[1]] = maskval","outputs":[]},{"cell_type":"code","metadata":{"tags":[]},"source":"dot_p.size()","outputs":[{"output_type":"execute_result","execution_count":129,"data":{"text/plain":"torch.Size([8, 128, 128])"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[]},"source":"# dot_p","outputs":[]},{"cell_type":"code","metadata":{"tags":[]},"source":"dot = F.softmax(dot_p, dim = 2)","outputs":[]},{"cell_type":"code","metadata":{"tags":[]},"source":"dot.size()","outputs":[{"output_type":"execute_result","execution_count":133,"data":{"text/plain":"torch.Size([8, 128, 128])"},"metadata":{}}]},{"cell_type":"markdown","source":"The computed self-attention is now being applied to the values vector","metadata":{"tags":[]}},{"cell_type":"code","metadata":{"tags":[]},"source":"out = torch.bmm(dot, values).view(b, heads, t, e)","outputs":[]},{"cell_type":"code","metadata":{"tags":[]},"source":"out.size()","outputs":[{"output_type":"execute_result","execution_count":135,"data":{"text/plain":"torch.Size([1, 8, 128, 256])"},"metadata":{}}]},{"cell_type":"markdown","source":"All the _eight_ heads are back and now we need to unify them using a linear layer ","metadata":{"tags":[]}},{"cell_type":"markdown","source":"We need to swap back the heads with the batch size to unify all the heads","metadata":{"tags":[]}},{"cell_type":"code","metadata":{"tags":[]},"source":"out = out.transpose(1, 2).contiguous().view(b, t, heads * emb_dim)","outputs":[]},{"cell_type":"code","metadata":{"tags":[]},"source":"out.size()","outputs":[{"output_type":"execute_result","execution_count":139,"data":{"text/plain":"torch.Size([1, 128, 2048])"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[]},"source":"unifyheads = nn.Linear(emb_dim * heads, emb_dim)","outputs":[]},{"cell_type":"code","metadata":{"tags":[]},"source":"input_tnsr_tfmb = unifyheads(out)","outputs":[]},{"cell_type":"code","metadata":{"tags":[]},"source":"# output from the self-attention block\ninput_tnsr_tfmb.size()","outputs":[{"output_type":"execute_result","execution_count":142,"data":{"text/plain":"torch.Size([1, 128, 256])"},"metadata":{}}]},{"cell_type":"markdown","source":"# Transformer Block","metadata":{"tags":[]}},{"cell_type":"markdown","source":"The **Transformer Block** consists of _n_ **Transformers** in a _Sequential_ order. \n\nHere, we will look at a single such **Transformer**","metadata":{"tags":[]}},{"cell_type":"markdown","source":"<a href=\"https://imgur.com/xSJD2TR\"><img src=\"https://i.imgur.com/xSJD2TR.png\" title=\"source: imgur.com\" /></a>","metadata":{"tags":[]}},{"cell_type":"markdown","source":"A **Transformer** contains the following things,\n\n    1. Self-Attention (The one explored above)\n    2. LayerNorm-1\n    3. LayerNorm-2\n    4. Feed-Forward Network","metadata":{"tags":[]}},{"cell_type":"markdown","source":"The feed-forward network contains a **`feed_forward_multiplication_factor`**","metadata":{"tags":[]}},{"cell_type":"code","metadata":{"tags":[]},"source":"feed_forward_mult = 3\nnorm_1 = nn.LayerNorm(emb_dim)\nnorm_2 = nn.LayerNorm(emb_dim)\nfeed_forward = nn.Sequential(\n    nn.Linear(emb_dim, feed_forward_mult * emb_dim),\n    nn.ReLU(),\n    nn.Linear(feed_forward_mult*emb_dim, emb_dim)\n\n)","outputs":[]},{"cell_type":"code","metadata":{"tags":[]},"source":"attention_output = input_tnsr_tfmb","outputs":[]},{"cell_type":"code","metadata":{"tags":[]},"source":"attention_output.size()","outputs":[{"output_type":"execute_result","execution_count":147,"data":{"text/plain":"torch.Size([1, 128, 256])"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[]},"source":"op_ = norm_1(attention_output)","outputs":[]},{"cell_type":"code","metadata":{"tags":[]},"source":"op_.size()","outputs":[{"output_type":"execute_result","execution_count":149,"data":{"text/plain":"torch.Size([1, 128, 256])"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[]},"source":"op_ = feed_forward(op_)","outputs":[]},{"cell_type":"code","metadata":{"tags":[]},"source":"op_.size()","outputs":[{"output_type":"execute_result","execution_count":151,"data":{"text/plain":"torch.Size([1, 128, 256])"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[]},"source":"op_ = norm_2(op_)","outputs":[]},{"cell_type":"code","metadata":{"tags":[]},"source":"op_.size()","outputs":[{"output_type":"execute_result","execution_count":153,"data":{"text/plain":"torch.Size([1, 128, 256])"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[]},"source":"# Output from transformer block\nop_","outputs":[{"output_type":"execute_result","execution_count":154,"data":{"text/plain":"tensor([[[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n         [-1.0195,  0.0225,  0.9978,  ..., -0.9235, -1.5130, -0.2702],\n         [-0.4327, -1.2847,  0.7194,  ...,  0.2320, -1.9380, -0.3782],\n         ...,\n         [-0.8535,  0.1518,  2.2584,  ..., -0.1849, -1.5483,  1.2892],\n         [-1.4051,  0.1749,  1.5635,  ..., -0.1841, -1.1818,  0.5500],\n         [-0.4399,  0.8001,  2.0931,  ..., -0.5775, -0.9920,  0.6277]]],\n       grad_fn=<NativeLayerNormBackward>)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Generator Block","metadata":{"tags":[]}},{"cell_type":"markdown","source":"The **Generator Block** contains the following, \n\n    1. Token Embedding (Used earlier in the Self-Attention)\n    2. Position Embedding (Used earlier in the Self-Attention)\n    3. TransformerBlock (Here we will be using only one Transformer and this can be adjusted with the **depth** parameter)\n    4. Probabilities Linear\n","metadata":{"tags":[]}},{"cell_type":"markdown","source":"<a href=\"https://imgur.com/nJf7hYk\"><img src=\"https://i.imgur.com/nJf7hYk.png\" title=\"source: imgur.com\" /></a>","metadata":{"tags":[]}},{"cell_type":"markdown","source":"Here, we are taking `seq_length=128` i.e. _128_ tokens will be generated. Therefore, we map the output from the **Transformer Block** using a _Linear Layer_ to 1000 neurons. ","metadata":{"tags":[]}},{"cell_type":"markdown","source":"Now we take a `log_softmax` over the output of the _Linear Layer_ on the second dimension. ","metadata":{"tags":[]}},{"cell_type":"code","metadata":{"tags":[]},"source":"toprobabilities = nn.Linear(emb_dim, num_tokens)d","outputs":[]},{"cell_type":"code","metadata":{"tags":[]},"source":"out_probs = toprobabilities(op_.view(b*t, e))","outputs":[]},{"cell_type":"code","metadata":{"tags":[]},"source":"out_probs.size()","outputs":[{"output_type":"execute_result","execution_count":157,"data":{"text/plain":"torch.Size([128, 1000])"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[]},"source":"out_probs = out_probs.view(b, t, num_tokens)","outputs":[]},{"cell_type":"code","metadata":{"tags":[]},"source":"out_probs.size()","outputs":[{"output_type":"execute_result","execution_count":159,"data":{"text/plain":"torch.Size([1, 128, 1000])"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[]},"source":"all_op_tokens = F.log_softmax(out_probs, dim = 2)","outputs":[]},{"cell_type":"code","metadata":{"tags":[]},"source":"all_op_tokens.size()","outputs":[{"output_type":"execute_result","execution_count":162,"data":{"text/plain":"torch.Size([1, 128, 1000])"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[]},"source":"len((torch.topk(all_op_tokens, k = 1, dim = 2))[1].flatten().tolist())","outputs":[{"output_type":"execute_result","execution_count":182,"data":{"text/plain":"128"},"metadata":{}}]},{"cell_type":"markdown","source":"# Classification Block","metadata":{"tags":[]}},{"cell_type":"markdown","source":"<a href=\"https://imgur.com/n0KeriK\"><img src=\"https://i.imgur.com/n0KeriK.png\" title=\"source: imgur.com\" /></a>","metadata":{"tags":[]}},{"cell_type":"code","metadata":{"tags":[]},"source":"max_pool = True","outputs":[]},{"cell_type":"code","metadata":{"tags":[]},"source":"NUM_CLASSES = 20","outputs":[]},{"cell_type":"code","metadata":{"tags":[]},"source":"toprobabilities = nn.Linear(emb_dim, NUM_CLASSES)","outputs":[]},{"cell_type":"code","metadata":{"tags":[]},"source":"tt_ = op_.max(dim=1)[0] if max_pool else op_.mean(dim=1)","outputs":[]},{"cell_type":"code","metadata":{"tags":[]},"source":"tt_.size()","outputs":[{"output_type":"execute_result","execution_count":207,"data":{"text/plain":"torch.Size([1, 256])"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[]},"source":"tt_ = toprobabilities(tt_)","outputs":[]},{"cell_type":"code","metadata":{"tags":[]},"source":"tt_.size()","outputs":[{"output_type":"execute_result","execution_count":209,"data":{"text/plain":"torch.Size([1, 20])"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[]},"source":"tt_ = F.log_softmax(tt_, dim = 1)","outputs":[]},{"cell_type":"code","metadata":{"tags":[]},"source":"tt_.size()","outputs":[{"output_type":"execute_result","execution_count":211,"data":{"text/plain":"torch.Size([1, 20])"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[]},"source":"# max_pool = Flase \ntorch.topk(tt_, 3)","outputs":[{"output_type":"execute_result","execution_count":195,"data":{"text/plain":"torch.return_types.topk(\nvalues=tensor([[nan, nan, nan]], grad_fn=<TopkBackward>),\nindices=tensor([[12, 14, 13]]))"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[]},"source":"# max_pool = True\ntorch.topk(tt_, 3)","outputs":[{"output_type":"execute_result","execution_count":212,"data":{"text/plain":"torch.return_types.topk(\nvalues=tensor([[nan, nan, nan]], grad_fn=<TopkBackward>),\nindices=tensor([[12, 14, 13]]))"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[]},"source":"","outputs":[],"execution_count":null}],"nbformat":4,"nbformat_minor":2,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"},"deepnote_execution_queue":[]}}