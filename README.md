The Notebook provides a breif hands-on experience and explanation regarding the data transformations and inner workings of a Multi Headed Self-Attention Transformer as described in the paper **Attention is All you need** (https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf "Research paper")

To keep things simple, the whole network/architecture is broken down into three parts,

	1. Self Attention Transformer
	2. Transformer Block
	3. Generator/Classification Block

The  notebook is also available here (https://beta.deepnote.com/publish/0a4be21b-db44-4155-9a75-329754bc53b3-3e50bb5b-3d33-4a56-98a9-bb1b5ca55e50 "Transformer Breakdown")

